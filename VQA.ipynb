{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3887986,"sourceType":"datasetVersion","datasetId":2310141}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©\n# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©\nimport os\nimport torch\nimport numpy as np\nfrom datasets import load_dataset\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nfrom transformers import ViltProcessor, ViltForQuestionAnswering, AutoProcessor\nfrom transformers import TrainingArguments, Trainer\nimport json\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport random\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:12:48.348491Z","iopub.execute_input":"2025-02-28T04:12:48.348750Z","iopub.status.idle":"2025-02-28T04:13:09.977224Z","shell.execute_reply.started":"2025-02-28T04:12:48.348725Z","shell.execute_reply":"2025-02-28T04:13:09.976535Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¨Ø°Ø±Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ø¶Ù…Ø§Ù† Ø¥Ù…ÙƒØ§Ù†ÙŠØ© ØªÙƒØ±Ø§Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \nset_seed(42)\n\n# ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:13:38.689111Z","iopub.execute_input":"2025-02-28T04:13:38.689438Z","iopub.status.idle":"2025-02-28T04:13:38.767887Z","shell.execute_reply.started":"2025-02-28T04:13:38.689409Z","shell.execute_reply":"2025-02-28T04:13:38.766977Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\n# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø£ÙƒØ«Ø± ØªÙ‚Ø¯Ù…Ù‹Ø§ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡\nmodel_name = \"dandelin/vilt-b32-mlm\"  # Ù†Ù…ÙˆØ°Ø¬ Ø£Ø³Ø§Ø³ÙŠ Ø£ÙØ¶Ù„\n\n# ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨\nprocessor = AutoProcessor.from_pretrained(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:13:43.483967Z","iopub.execute_input":"2025-02-28T04:13:43.484284Z","iopub.status.idle":"2025-02-28T04:13:45.085556Z","shell.execute_reply.started":"2025-02-28T04:13:43.484256Z","shell.execute_reply":"2025-02-28T04:13:45.084661Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51737963a4ec4e7d8ee56da9a21c61fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae6cf8674c3249459d046a2c2f75437c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44168f3abb98475a8bccf84c92936ebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36270f5a55c3477f9734de0245bd8c12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a2aa3ddee2741a9941214f70960d997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"349e7ae0b8f74d34a6b058bc35499c06"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª JSON\njson_path = \"/kaggle/input/vizwiz/Annotations/Annotations/train.json\"\nimage_folder = \"/kaggle/input/vizwiz/train/train/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:13:51.589747Z","iopub.execute_input":"2025-02-28T04:13:51.590064Z","iopub.status.idle":"2025-02-28T04:13:51.593748Z","shell.execute_reply.started":"2025-02-28T04:13:51.590040Z","shell.execute_reply":"2025-02-28T04:13:51.592684Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªÙ†Ø¸ÙŠÙÙ‡Ø§\nprint(\"Loading dataset from JSON...\")\ntry:\n    train_dataset = load_dataset(\"json\", data_files=json_path)[\"train\"]\n    print(f\"Dataset loaded successfully with {len(train_dataset)} examples\")\n    \n    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø°Ø§Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ© Ø£Ùˆ unanswerable\n    def is_valid_example(example):\n        if \"answers\" not in example or not example[\"answers\"]:\n            return False\n            \n        if isinstance(example[\"answers\"], list):\n            for answer in example[\"answers\"]:\n                answer_text = answer.get(\"answer\", \"\") if isinstance(answer, dict) else answer\n                if answer_text and answer_text != \"unanswerable\":\n                    return True\n        return False\n    \n    train_dataset = train_dataset.filter(is_valid_example)\n    print(f\"Dataset cleaned: {len(train_dataset)} valid examples remaining\")\n    \nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    raise\n\n# ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Augmentation)\ndef augment_example(example):\n    \"\"\"ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù…Ø«Ù„Ø©\"\"\"\n    # Ù†Ø³Ø®Ø© Ø·Ø¨Ù‚ Ø§Ù„Ø£ØµÙ„ Ù…Ù† Ø§Ù„Ù…Ø«Ø§Ù„\n    return example\n\n# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ ØªØ­Ø³ÙŠÙ† ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª\nprint(\"Splitting dataset...\")\ntrain_val_split = train_dataset.train_test_split(test_size=0.15, seed=42)\ntrain_data = train_val_split[\"train\"]\nval_data = train_val_split[\"test\"]\n\n# ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª\nprint(\"Analyzing answer distribution...\")\nanswer_counts = {}\nfor example in train_data:\n    if \"answers\" in example and example[\"answers\"]:\n        for answer in example[\"answers\"]:\n            answer_text = answer.get(\"answer\", \"\") if isinstance(answer, dict) else answer\n            if answer_text and answer_text != \"unanswerable\":\n                answer_counts[answer_text] = answer_counts.get(answer_text, 0) + 1\n\n# ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø­Ø³Ø¨ ØªÙƒØ±Ø§Ø±Ù‡Ø§\nsorted_answers = sorted(answer_counts.items(), key=lambda x: x[1], reverse=True)\nprint(f\"Top 10 answers: {sorted_answers[:10]}\")\n\n# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ù‹Ø§ ÙÙ‚Ø· (Ù„Ù„Ø­Ø¯ Ù…Ù† Ø§Ù„ÙØ¦Ø§Øª ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø©)\nmin_answer_freq = 3  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªÙƒØ±Ø§Ø±\nanswer_list = [answer for answer, count in sorted_answers if count >= min_answer_freq]\nprint(f\"Using {len(answer_list)} answers that appear at least {min_answer_freq} times\")\n\n# Ø¥Ø¶Ø§ÙØ© ÙØ¦Ø© \"Ø£Ø®Ø±Ù‰\" Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù†Ø§Ø¯Ø±Ø©\nanswer_list.append(\"other\")\n\n# Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª\nanswer2id = {answer: idx for idx, answer in enumerate(answer_list)}\nid2answer = {idx: answer for answer, idx in answer2id.items()}\n\n# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ\nmodel = ViltForQuestionAnswering.from_pretrained(model_name)\nmodel.config.id2label = id2answer\nmodel.config.label2id = answer2id\n# Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØµÙ†Ù Ø¨Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯\nclassifier_dropout = 0.1  # Ø¥Ø¶Ø§ÙØ© dropout Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù€ overfitting\nhidden_size = model.config.hidden_size\nnum_labels = len(answer2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:14:02.799668Z","iopub.execute_input":"2025-02-28T04:14:02.799992Z","iopub.status.idle":"2025-02-28T04:14:10.645548Z","shell.execute_reply.started":"2025-02-28T04:14:02.799968Z","shell.execute_reply":"2025-02-28T04:14:10.644434Z"}},"outputs":[{"name":"stdout","text":"Loading dataset from JSON...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b131866cf3c24e58ad36070460a26ddd"}},"metadata":{}},{"name":"stdout","text":"Dataset loaded successfully with 20523 examples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/20523 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e3a3d3e62f74290a56c21b0b7a4811a"}},"metadata":{}},{"name":"stdout","text":"Dataset cleaned: 20484 valid examples remaining\nSplitting dataset...\nAnalyzing answer distribution...\nTop 10 answers: [('unsuitable', 21493), ('no', 4511), ('yes', 3959), ('white', 2063), ('grey', 1923), ('black', 1713), ('blue', 1562), ('red', 1011), ('pink', 721), ('brown', 703)]\nUsing 6523 answers that appear at least 3 times\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c272bc10238d413a934e9b0adc8f5843"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.1.bias', 'classifier.1.weight', 'classifier.3.bias', 'classifier.3.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØµÙ†Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø¨Ù‚Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©\nclass EnhancedClassifier(nn.Module):\n    def __init__(self, hidden_size, num_labels, dropout_prob=0.1):\n        super().__init__()\n        self.dense1 = nn.Linear(hidden_size, hidden_size)\n        self.dropout1 = nn.Dropout(dropout_prob)\n        self.dense2 = nn.Linear(hidden_size, hidden_size // 2)\n        self.dropout2 = nn.Dropout(dropout_prob)\n        self.classifier = nn.Linear(hidden_size // 2, num_labels)\n        \n    def forward(self, x):\n        x = self.dropout1(F.gelu(self.dense1(x)))\n        x = self.dropout2(F.gelu(self.dense2(x)))\n        return self.classifier(x)\n\n# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…ØµÙ†Ù Ø§Ù„Ù…Ø­Ø³Ù†\nmodel.classifier = EnhancedClassifier(hidden_size, num_labels, classifier_dropout)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:14:25.635950Z","iopub.execute_input":"2025-02-28T04:14:25.636270Z","iopub.status.idle":"2025-02-28T04:14:26.059403Z","shell.execute_reply.started":"2025-02-28T04:14:25.636244Z","shell.execute_reply":"2025-02-28T04:14:26.058597Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"ViltForQuestionAnswering(\n  (vilt): ViltModel(\n    (embeddings): ViltEmbeddings(\n      (text_embeddings): TextEmbeddings(\n        (word_embeddings): Embedding(30522, 768)\n        (position_embeddings): Embedding(40, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (patch_embeddings): ViltPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n      )\n      (token_type_embeddings): Embedding(2, 768)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViltEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViltLayer(\n          (attention): ViltAttention(\n            (attention): ViltSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViltSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViltIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViltOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViltPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (classifier): EnhancedClassifier(\n    (dense1): Linear(in_features=768, out_features=768, bias=True)\n    (dropout1): Dropout(p=0.1, inplace=False)\n    (dense2): Linear(in_features=768, out_features=384, bias=True)\n    (dropout2): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=384, out_features=6524, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ØªØ­Ø³ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©\ndef load_image(image_name):\n    image_path = os.path.join(image_folder, image_name)\n    if os.path.exists(image_path):\n        try:\n            img = Image.open(image_path).convert(\"RGB\")\n            \n            # ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©\n            img = img.resize((224, 224))\n            \n            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ù‚Ù„ÙŠÙ„Ø§Ù‹\n            from PIL import ImageEnhance\n            enhancer = ImageEnhance.Contrast(img)\n            img = enhancer.enhance(1.1)\n            \n            return img\n        except Exception as e:\n            print(f\"Error loading image {image_name}: {e}\")\n            return Image.new(\"RGB\", (224, 224), color=\"white\")\n    else:\n        print(f\"âš ï¸ Warning: Image not found - {image_name}\")\n        return Image.new(\"RGB\", (224, 224), color=\"white\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:14:37.571048Z","iopub.execute_input":"2025-02-28T04:14:37.571341Z","iopub.status.idle":"2025-02-28T04:14:37.576498Z","shell.execute_reply.started":"2025-02-28T04:14:37.571318Z","shell.execute_reply":"2025-02-28T04:14:37.575672Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ØªØ¹Ø±ÙŠÙ Ù…Ø¹Ø§Ù„Ø¬ Ø®Ø§Øµ Ù…Ø­Ø³Ù† Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n@dataclass\nclass CustomDataCollator:\n    processor: any\n    \n    def __call__(self, features):\n        if not features:\n            return {}\n            \n        batch = {}\n        \n        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø­Ù‚ÙˆÙ„\n        if \"pixel_values\" in features[0]:\n            batch[\"pixel_values\"] = torch.stack([f[\"pixel_values\"] for f in features])\n        \n        for field in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n            if field in features[0]:\n                # Ø§Ø³ØªØ®Ø¯Ø§Ù… padding Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø©\n                values = [f[field] for f in features]\n                max_length = max(len(v) for v in values)\n                \n                padded_values = []\n                for v in values:\n                    padding = [0] * (max_length - len(v))\n                    padded_values.append(v + padding)\n                \n                batch[field] = torch.tensor(padded_values)\n        \n        if \"labels\" in features[0]:\n            batch[\"labels\"] = torch.tensor([f[\"labels\"] for f in features])\n        \n        return batch\n\n# Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø³Ù†Ø©\ndef preprocess_function(examples):\n    valid_questions = []\n    valid_images = []\n    valid_labels = []\n    \n    for i in range(len(examples.get(\"question\", []))):\n        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n        if \"image\" not in examples or i >= len(examples[\"image\"]) or \"answers\" not in examples or i >= len(examples[\"answers\"]):\n            continue\n            \n        image_name = examples[\"image\"][i]\n        img = load_image(image_name)\n        \n        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª\n        has_valid_answer = False\n        if isinstance(examples[\"answers\"][i], list) and examples[\"answers\"][i]:\n            for answer in examples[\"answers\"][i]:\n                answer_text = answer.get(\"answer\", \"\") if isinstance(answer, dict) else answer\n                if answer_text and answer_text != \"unanswerable\":\n                    if answer_text in answer2id:\n                        valid_labels.append(answer2id[answer_text])\n                    else:\n                        # Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙØ¦Ø© \"Ø£Ø®Ø±Ù‰\" Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù†Ø§Ø¯Ø±Ø©\n                        valid_labels.append(answer2id[\"other\"])\n                    has_valid_answer = True\n                    break\n        \n        if has_valid_answer:\n            valid_questions.append(examples[\"question\"][i])\n            valid_images.append(img)\n    \n    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ØµØ§Ù„Ø­Ø©\n    if not valid_questions:\n        return {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": [], \"labels\": []}\n    \n    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n    try:\n        encoding = processor(\n            images=valid_images,\n            text=valid_questions,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ³Ù…ÙŠØ§Øª\n        encoding[\"labels\"] = valid_labels\n        \n        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙ†Ø³ÙˆØ±Ø§Øª Ø¥Ù„Ù‰ Ù‚ÙˆØ§Ø¦Ù…\n        result = {k: v.tolist() if isinstance(v, torch.Tensor) else v for k, v in encoding.items()}\n        return result\n    except Exception as e:\n        print(f\"Error in preprocessing: {e}\")\n        return {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": [], \"labels\": []}\n\n# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø©\nprint(\"Preparing datasets for training...\")\ntrain_sample_size = min(8000, len(train_data))  # Ø²ÙŠØ§Ø¯Ø© Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨\neval_sample_size = min(1200, len(val_data))\n\n# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ÙˆØ²ÙˆÙ†Ø© Ù„ØªØ­Ø³ÙŠÙ† ØªÙ…Ø«ÙŠÙ„ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù†Ø§Ø¯Ø±Ø©\nweighted_train_indices = []\nanswer_probabilities = {}\n\n# Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ø®ØªÙŠØ§Ø± ÙƒÙ„ Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ Ø¹ÙƒØ³ÙŠ Ù…Ø¹ ØªÙƒØ±Ø§Ø±Ù‡Ø§\ntotal_answers = sum(answer_counts.values())\nfor answer, count in answer_counts.items():\n    answer_probabilities[answer] = 1.0 / (count / total_answers)\n\n# ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª\nmax_prob = max(answer_probabilities.values())\nfor answer in answer_probabilities:\n    answer_probabilities[answer] /= max_prob\n\n# Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…ÙˆØ²ÙˆÙ†Ø©\nfor idx, example in enumerate(train_data):\n    if idx >= len(train_data):\n        break\n    \n    if \"answers\" in example and example[\"answers\"]:\n        for answer in example[\"answers\"]:\n            answer_text = answer.get(\"answer\", \"\") if isinstance(answer, dict) else answer\n            if answer_text in answer_probabilities:\n                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¤Ø´Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©\n                if random.random() < answer_probabilities[answer_text]:\n                    weighted_train_indices.append(idx)\n                    break\n\n# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù„Ø¯ÙŠÙ†Ø§ Ù…Ø§ ÙŠÙƒÙÙŠ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\nif len(weighted_train_indices) < train_sample_size:\n    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¤Ø´Ø±Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±\n    additional_indices = random.sample(\n        [i for i in range(len(train_data)) if i not in weighted_train_indices],\n        min(train_sample_size - len(weighted_train_indices), len(train_data) - len(weighted_train_indices))\n    )\n    weighted_train_indices.extend(additional_indices)\n\n# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰\nweighted_train_indices = weighted_train_indices[:train_sample_size]\neval_indices = random.sample(range(len(val_data)), eval_sample_size)\n\nsmall_train_dataset = train_data.select(weighted_train_indices)\nsmall_eval_dataset = val_data.select(eval_indices)\n\nprint(f\"Selected {len(small_train_dataset)} examples for training\")\nprint(f\"Selected {len(small_eval_dataset)} examples for evaluation\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:14:58.563560Z","iopub.execute_input":"2025-02-28T04:14:58.563918Z","iopub.status.idle":"2025-02-28T04:15:01.309933Z","shell.execute_reply.started":"2025-02-28T04:14:58.563894Z","shell.execute_reply":"2025-02-28T04:15:01.309199Z"}},"outputs":[{"name":"stdout","text":"Preparing datasets for training...\nSelected 8000 examples for training\nSelected 1200 examples for evaluation\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\nprint(\"Preprocessing training data...\")\ntrain_dataset = small_train_dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=4,  # ØªØ­Ø³ÙŠÙ† Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©\n    remove_columns=train_data.column_names\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:15:06.163632Z","iopub.execute_input":"2025-02-28T04:15:06.163963Z","iopub.status.idle":"2025-02-28T04:31:20.028526Z","shell.execute_reply.started":"2025-02-28T04:15:06.163936Z","shell.execute_reply":"2025-02-28T04:31:20.027486Z"}},"outputs":[{"name":"stdout","text":"Preprocessing training data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2457707813934434a3703fbb6f0360a3"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"print(\"Preprocessing evaluation data...\")\neval_dataset = small_eval_dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=4,\n    remove_columns=val_data.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:34:44.484059Z","iopub.execute_input":"2025-02-28T04:34:44.484418Z","iopub.status.idle":"2025-02-28T04:37:07.550971Z","shell.execute_reply.started":"2025-02-28T04:34:44.484387Z","shell.execute_reply":"2025-02-28T04:37:07.550209Z"}},"outputs":[{"name":"stdout","text":"Preprocessing evaluation data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ee9431e590413ba6ad289760f2489a"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"\n# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\ntrain_dataset = train_dataset.filter(lambda x: len(x[\"input_ids\"]) > 0)\neval_dataset = eval_dataset.filter(lambda x: len(x[\"input_ids\"]) > 0)\n\nprint(f\"Preprocessed training examples: {len(train_dataset)}\")\nprint(f\"Preprocessed evaluation examples: {len(eval_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:38:20.952825Z","iopub.execute_input":"2025-02-28T04:38:20.953147Z","iopub.status.idle":"2025-02-28T05:21:35.595164Z","shell.execute_reply.started":"2025-02-28T04:38:20.953123Z","shell.execute_reply":"2025-02-28T05:21:35.594456Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b234ad4a15f5444e94addee7e4dd1dec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4369bfc73dca4d6eb7afd691a89ebff9"}},"metadata":{}},{"name":"stdout","text":"Preprocessed training examples: 8000\nPreprocessed evaluation examples: 1200\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install evaluate\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:23:56.930947Z","iopub.execute_input":"2025-02-28T05:23:56.931290Z","iopub.status.idle":"2025-02-28T05:24:02.309547Z","shell.execute_reply.started":"2025-02-28T05:23:56.931261Z","shell.execute_reply":"2025-02-28T05:24:02.308573Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n\n    # âœ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©\n    predicted_classes = np.argmax(logits, axis=1)\n\n    # âœ… ØªØ­ÙˆÙŠÙ„ `one-hot encoding` Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… ØµØ­ÙŠØ­Ø©\n    true_classes = np.argmax(labels, axis=1)\n\n    # âœ… Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©\n    acc = accuracy.compute(predictions=predicted_classes, references=true_classes)\n\n    return {\"accuracy\": acc[\"accuracy\"]}\n\nclass CustomDataCollator:\n    def __init__(self, processor, num_classes=6524):  # Ø­Ø¯Ø¯ Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª\n        self.processor = processor\n        self.num_classes = num_classes\n\n    def __call__(self, features):\n        batch = {}\n\n        batch[\"pixel_values\"] = torch.stack(\n            [torch.tensor(f[\"pixel_values\"]) if isinstance(f[\"pixel_values\"], list) else f[\"pixel_values\"]\n             for f in features]\n        )\n\n        batch[\"input_ids\"] = torch.stack(\n            [torch.tensor(f[\"input_ids\"]) if isinstance(f[\"input_ids\"], list) else f[\"input_ids\"]\n             for f in features]\n        )\n\n        batch[\"attention_mask\"] = torch.stack(\n            [torch.tensor(f[\"attention_mask\"]) if isinstance(f[\"attention_mask\"], list) else f[\"attention_mask\"]\n             for f in features]\n        )\n\n        # âœ… ØªØ­ÙˆÙŠÙ„ `labels` Ø¥Ù„Ù‰ One-Hot\n        labels = [f[\"labels\"] for f in features]\n        labels_tensor = torch.zeros((len(labels), self.num_classes))  # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© Ø£ØµÙØ§Ø±\n        labels_tensor.scatter_(1, torch.tensor(labels).unsqueeze(1), 1)  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ One-Hot\n\n        batch[\"labels\"] = labels_tensor\n\n        return batch\n\n\n\n# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\ndata_collator = CustomDataCollator(processor=processor)\n\n# ØªØ¹ÙŠÙŠÙ† Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨\ntraining_args = TrainingArguments(\n    output_dir=\"./vqa_finetuned_model\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    num_train_epochs=3,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    fp16=True,\n    gradient_accumulation_steps=4,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    report_to=\"none\",\n    remove_unused_columns=False,\n    warmup_steps=200,\n    dataloader_num_workers=2,\n    lr_scheduler_type=\"cosine\",\n)\n\nclass MixupTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.mixup_alpha = 0.2  # Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ù…Ø²Ø¬\n        \n    def training_step(self, model, inputs, num_items_in_batch):\n        \"\"\"ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù…Ø²Ø¬ Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨\"\"\"\n        if self.mixup_alpha > 0 and \"pixel_values\" in inputs and \"labels\" in inputs:\n            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø²Ø¬ Ø¨ÙŠÙ† Ø§Ù„ØµÙˆØ±\n            batch_size = inputs[\"pixel_values\"].size(0)\n            if batch_size > 1:  # Ù†Ø­ØªØ§Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ ØµÙˆØ±ØªÙŠÙ† Ù„Ù„Ù…Ø²Ø¬\n                # ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø²Ø¬\n                lam = np.random.beta(self.mixup_alpha, self.mixup_alpha, batch_size)\n                lam = torch.from_numpy(lam).float().to(inputs[\"pixel_values\"].device)\n                lam = lam.view(-1, 1, 1, 1)\n                \n                # ØªØ´ÙˆÙŠØ´ Ø§Ù„ÙÙ‡Ø§Ø±Ø³\n                index = torch.randperm(batch_size).to(inputs[\"pixel_values\"].device)\n                \n                # Ù…Ø²Ø¬ Ø§Ù„ØµÙˆØ±\n                mixed_pixel_values = lam * inputs[\"pixel_values\"] + (1 - lam) * inputs[\"pixel_values\"][index, :]\n                inputs[\"pixel_values\"] = mixed_pixel_values\n                \n                # Ù„Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ù…Ø²Ø¬ Ø§Ù„ØªØ³Ù…ÙŠØ§ØªØŒ Ø¨Ù„ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ³Ù…ÙŠØ© Ø§Ù„Ø£ØµÙ„ÙŠØ© (Ù‡Ø°Ø§ ÙŠÙƒÙÙŠ Ù„Ù…Ù‡Ù…Ø© VQA)\n        \n        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ input_ids Ø£Ùˆ inputs_embeds\n        if \"input_ids\" not in inputs and \"inputs_embeds\" not in inputs:\n            raise ValueError(\"ÙŠØ¬Ø¨ ØªÙˆÙÙŠØ± input_ids Ø£Ùˆ inputs_embeds Ù„Ù„Ù†Ù…ÙˆØ°Ø¬\")\n            \n        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© training_step Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØªÙ…Ø±ÙŠØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n        return super().training_step(model, inputs, num_items_in_batch)\n\n\n\n# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¯Ø±Ø¨\ntrainer = MixupTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\n# âœ… ØªØ¬Ù…ÙŠØ¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø³ÙÙ„Ù‰ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\nfor name, param in model.vilt.named_parameters():\n    if \"encoder.layer.0\" in name or \"encoder.layer.1\" in name or \"encoder.layer.2\" in name:\n        param.requires_grad = False\n\n# Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:24:09.073058Z","iopub.execute_input":"2025-02-28T05:24:09.073398Z","iopub.status.idle":"2025-02-28T05:24:09.664741Z","shell.execute_reply.started":"2025-02-28T05:24:09.073371Z","shell.execute_reply":"2025-02-28T05:24:09.663919Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04fcffba68ac46dd9b7ce7b5903c48d7"}},"metadata":{}},{"name":"stdout","text":"Total parameters: 114,992,636\nTrainable parameters: 79,553,276 (69.18%)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(\"training ...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:24:42.044783Z","iopub.execute_input":"2025-02-28T05:24:42.045138Z","iopub.status.idle":"2025-02-28T07:07:15.466920Z","shell.execute_reply.started":"2025-02-28T05:24:42.045109Z","shell.execute_reply":"2025-02-28T07:07:15.466120Z"}},"outputs":[{"name":"stdout","text":"training ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 1:42:18, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>4348.181600</td>\n      <td>4297.930176</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>491.300700</td>\n      <td>254.207611</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>8.446400</td>\n      <td>6.272337</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>7.588500</td>\n      <td>5.873587</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>7.689600</td>\n      <td>5.678341</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>7.055200</td>\n      <td>5.717703</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>7.628700</td>\n      <td>5.682053</td>\n      <td>0.347500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=750, training_loss=953.8862706197103, metrics={'train_runtime': 6153.0314, 'train_samples_per_second': 3.901, 'train_steps_per_second': 0.122, 'total_flos': 527143380480000.0, 'train_loss': 953.8862706197103, 'epoch': 3.0})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ\nprint(\"Saving final model...\")\nmodel.save_pretrained(\"./vqa_finetuned_model_final\")\nprocessor.save_pretrained(\"./vqa_finetuned_model_final\")\nprint(\"Model saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:22:45.118140Z","iopub.execute_input":"2025-02-28T07:22:45.118541Z","iopub.status.idle":"2025-02-28T07:22:46.572053Z","shell.execute_reply.started":"2025-02-28T07:22:45.118512Z","shell.execute_reply":"2025-02-28T07:22:46.571238Z"}},"outputs":[{"name":"stdout","text":"Saving final model...\nModel saved successfully!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ÙƒÙˆØ¯ Ù…Ø­Ø³Ù† Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\ndef test_model(image_name, question):\n    img = load_image(image_name)\n    inputs = processor(img, question, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ 3 Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø­ØªÙ…Ù„Ø©\n    logits = outputs.logits\n    probs = F.softmax(logits, dim=-1)[0]\n    top_probs, top_indices = probs.topk(3)\n    \n    results = []\n    for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):\n        answer = model.config.id2label.get(idx, \"ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ\")\n        results.append((answer, prob * 100))\n    \n    print(f\"Ø³Ø¤Ø§Ù„: {question}\")\n    print(f\"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø­ØªÙ…Ø§Ù„Ø§: {results[0][0]} (Ø§Ù„Ø«Ù‚Ø©: {results[0][1]:.2f}%)\")\n    print(f\"Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø®Ø±Ù‰ Ù…Ø­ØªÙ…Ù„Ø©: {results[1][0]} ({results[1][1]:.2f}%), {results[2][0]} ({results[2][1]:.2f}%)\")\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:24:18.597055Z","iopub.execute_input":"2025-02-28T07:24:18.597404Z","iopub.status.idle":"2025-02-28T07:24:18.604214Z","shell.execute_reply.started":"2025-02-28T07:24:18.597379Z","shell.execute_reply":"2025-02-28T07:24:18.603303Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n# Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\ndef evaluate_model_accuracy(dataset, num_samples=100):\n    if len(dataset) == 0:\n        print(\"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ‚ÙŠÙŠÙ…\")\n        return 0\n    \n    # Ø§Ø®ØªÙŠØ§Ø± Ø¹ÙŠÙ†Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ù„ØªÙ‚ÙŠÙŠÙ…\n    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n    correct = 0\n    \n    for idx in indices:\n        example = dataset[idx]\n        if \"image\" not in example or \"question\" not in example or \"answers\" not in example:\n            continue\n            \n        img = load_image(example[\"image\"])\n        question = example[\"question\"]\n        \n        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©\n        correct_answer = None\n        if isinstance(example[\"answers\"], list) and example[\"answers\"]:\n            answer = example[\"answers\"][0]\n            correct_answer = answer.get(\"answer\", \"\") if isinstance(answer, dict) else answer\n        \n        if not correct_answer or correct_answer == \"unanswerable\":\n            continue\n            \n        # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©\n        inputs = processor(img, question, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=40)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n        \n        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©\n        logits = outputs.logits\n        predicted_answer_id = logits.argmax(-1).item()\n        predicted_answer = model.config.id2label.get(predicted_answer_id, \"ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ\")\n        \n        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ø¨Ø§Ù„ØµØ­ÙŠØ­Ø©\n        if predicted_answer.lower() == correct_answer.lower():\n            correct += 1\n    \n    accuracy = correct / len(indices) * 100\n    print(f\"Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ {len(indices)} Ø¹ÙŠÙ†Ø©: {accuracy:.2f}%\")\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:29:24.304407Z","iopub.execute_input":"2025-02-28T07:29:24.304772Z","iopub.status.idle":"2025-02-28T07:29:24.312365Z","shell.execute_reply.started":"2025-02-28T07:29:24.304745Z","shell.execute_reply":"2025-02-28T07:29:24.311425Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\nprint(\"Evaluating final model...\")\nfinal_accuracy = evaluate_model_accuracy(val_data, num_samples=200)\nprint(f\"Final model accuracy: {final_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:29:27.563149Z","iopub.execute_input":"2025-02-28T07:29:27.563470Z","iopub.status.idle":"2025-02-28T07:29:40.297744Z","shell.execute_reply.started":"2025-02-28T07:29:27.563446Z","shell.execute_reply":"2025-02-28T07:29:40.296747Z"}},"outputs":[{"name":"stdout","text":"Evaluating final model...\nØ¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ 200 Ø¹ÙŠÙ†Ø©: 4.00%\nFinal model accuracy: 4.00%\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"for i in range(5):\n    sample = val_data[i]\n    print(f\"ğŸ”¹ Ø§Ù„Ø³Ø¤Ø§Ù„: {sample['question']}\")\n    print(f\"âœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©: {sample['answers']}\")\n    print(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:31:50.358539Z","iopub.execute_input":"2025-02-28T07:31:50.358937Z","iopub.status.idle":"2025-02-28T07:31:50.369110Z","shell.execute_reply.started":"2025-02-28T07:31:50.358906Z","shell.execute_reply":"2025-02-28T07:31:50.368237Z"}},"outputs":[{"name":"stdout","text":"ğŸ”¹ Ø§Ù„Ø³Ø¤Ø§Ù„: what number is the needle pointing to?\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©: [{'answer': 'illegible', 'answer_confidence': 'no'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'blurry', 'answer_confidence': 'no'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'no'}]\n--------------------------------------------------\nğŸ”¹ Ø§Ù„Ø³Ø¤Ø§Ù„: What color is the keyboard?\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©: [{'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'keyboard', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}]\n--------------------------------------------------\nğŸ”¹ Ø§Ù„Ø³Ø¤Ø§Ù„: What is in this bottle? What is in this bottle?\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©: [{'answer': 'ball park mustard', 'answer_confidence': 'yes'}, {'answer': 'efrwqerf', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'yes'}, {'answer': 'ball park mustard', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'yes'}, {'answer': 'bertman original ball park mustard', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'maybe'}]\n--------------------------------------------------\nğŸ”¹ Ø§Ù„Ø³Ø¤Ø§Ù„: What is it say on this tag?\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©: [{'answer': 'blank', 'answer_confidence': 'yes'}, {'answer': 'nothing', 'answer_confidence': 'maybe'}, {'answer': 'nothing', 'answer_confidence': 'yes'}, {'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'nothing', 'answer_confidence': 'yes'}, {'answer': 'nothing', 'answer_confidence': 'yes'}, {'answer': 'nothing to say', 'answer_confidence': 'yes'}, {'answer': 'nothing', 'answer_confidence': 'maybe'}, {'answer': 'nothing', 'answer_confidence': 'maybe'}, {'answer': 'nothing', 'answer_confidence': 'maybe'}]\n--------------------------------------------------\nğŸ”¹ Ø§Ù„Ø³Ø¤Ø§Ù„: Yes, I just need a description of the label on this bottle. Please be as specific as possible. Thank you.\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©: [{'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'blurry', 'answer_confidence': 'maybe'}, {'answer': 'lots information', 'answer_confidence': 'maybe'}, {'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'no'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'too blurry to read but label light blue white has many words on', 'answer_confidence': 'maybe'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}]\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!zip -r vqa_finetuned_model_final.zip ./vqa_finetuned_model_final\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'vqa_finetuned_model_final.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
