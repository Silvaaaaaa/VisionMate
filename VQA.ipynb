{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3887986,"sourceType":"datasetVersion","datasetId":2310141}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# استيراد المكتبات اللازمة\n# استيراد المكتبات اللازمة\nimport os\nimport torch\nimport numpy as np\nfrom datasets import load_dataset\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nfrom transformers import ViltProcessor, ViltForQuestionAnswering, AutoProcessor\nfrom transformers import TrainingArguments, Trainer\nimport json\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport random\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:12:48.348491Z","iopub.execute_input":"2025-02-28T04:12:48.348750Z","iopub.status.idle":"2025-02-28T04:13:09.977224Z","shell.execute_reply.started":"2025-02-28T04:12:48.348725Z","shell.execute_reply":"2025-02-28T04:13:09.976535Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# تعيين البذرة العشوائية لضمان إمكانية تكرار النتائج\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \nset_seed(42)\n\n# تعيين الأجهزة\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:13:38.689111Z","iopub.execute_input":"2025-02-28T04:13:38.689438Z","iopub.status.idle":"2025-02-28T04:13:38.767887Z","shell.execute_reply.started":"2025-02-28T04:13:38.689409Z","shell.execute_reply":"2025-02-28T04:13:38.766977Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\n# استخدام نموذج أكثر تقدمًا لتحسين الأداء\nmodel_name = \"dandelin/vilt-b32-mlm\"  # نموذج أساسي أفضل\n\n# تنزيل المعالج المناسب\nprocessor = AutoProcessor.from_pretrained(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:13:43.483967Z","iopub.execute_input":"2025-02-28T04:13:43.484284Z","iopub.status.idle":"2025-02-28T04:13:45.085556Z","shell.execute_reply.started":"2025-02-28T04:13:43.484256Z","shell.execute_reply":"2025-02-28T04:13:45.084661Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51737963a4ec4e7d8ee56da9a21c61fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae6cf8674c3249459d046a2c2f75437c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44168f3abb98475a8bccf84c92936ebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36270f5a55c3477f9734de0245bd8c12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a2aa3ddee2741a9941214f70960d997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"349e7ae0b8f74d34a6b058bc35499c06"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# تحميل بيانات JSON\njson_path = \"/kaggle/input/vizwiz/Annotations/Annotations/train.json\"\nimage_folder = \"/kaggle/input/vizwiz/train/train/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:13:51.589747Z","iopub.execute_input":"2025-02-28T04:13:51.590064Z","iopub.status.idle":"2025-02-28T04:13:51.593748Z","shell.execute_reply.started":"2025-02-28T04:13:51.590040Z","shell.execute_reply":"2025-02-28T04:13:51.592684Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# تحميل البيانات وتنظيفها\nprint(\"Loading dataset from JSON...\")\ntry:\n    train_dataset = load_dataset(\"json\", data_files=json_path)[\"train\"]\n    print(f\"Dataset loaded successfully with {len(train_dataset)} examples\")\n    \n    # تنظيف البيانات - إزالة الأمثلة ذات الإجابات الفارغة أو unanswerable\n    def is_valid_example(example):\n        if \"answers\" not in example or not example[\"answers\"]:\n            return False\n            \n        if isinstance(example[\"answers\"], list):\n            for answer in example[\"answers\"]:\n                answer_text = answer.get(\"answer\", \"\") if isinstance(answer, dict) else answer\n                if answer_text and answer_text != \"unanswerable\":\n                    return True\n        return False\n    \n    train_dataset = train_dataset.filter(is_valid_example)\n    print(f\"Dataset cleaned: {len(train_dataset)} valid examples remaining\")\n    \nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    raise\n\n# تعزيز البيانات (Data Augmentation)\ndef augment_example(example):\n    \"\"\"تطبيق تقنيات تعزيز البيانات على الأمثلة\"\"\"\n    # نسخة طبق الأصل من المثال\n    return example\n\n# تقسيم البيانات مع تحسين توزيع الفئات\nprint(\"Splitting dataset...\")\ntrain_val_split = train_dataset.train_test_split(test_size=0.15, seed=42)\ntrain_data = train_val_split[\"train\"]\nval_data = train_val_split[\"test\"]\n\n# تحليل توزيع الإجابات\nprint(\"Analyzing answer distribution...\")\nanswer_counts = {}\nfor example in train_data:\n    if \"answers\" in example and example[\"answers\"]:\n        for answer in example[\"answers\"]:\n            answer_text = answer.get(\"answer\", \"\") if isinstance(answer, dict) else answer\n            if answer_text and answer_text != \"unanswerable\":\n                answer_counts[answer_text] = answer_counts.get(answer_text, 0) + 1\n\n# ترتيب الإجابات حسب تكرارها\nsorted_answers = sorted(answer_counts.items(), key=lambda x: x[1], reverse=True)\nprint(f\"Top 10 answers: {sorted_answers[:10]}\")\n\n# اختيار الإجابات الأكثر شيوعًا فقط (للحد من الفئات وتحسين الدقة)\nmin_answer_freq = 3  # الحد الأدنى للتكرار\nanswer_list = [answer for answer, count in sorted_answers if count >= min_answer_freq]\nprint(f\"Using {len(answer_list)} answers that appear at least {min_answer_freq} times\")\n\n# إضافة فئة \"أخرى\" للإجابات النادرة\nanswer_list.append(\"other\")\n\n# إنشاء قاموس الإجابات\nanswer2id = {answer: idx for idx, answer in enumerate(answer_list)}\nid2answer = {idx: answer for answer, idx in answer2id.items()}\n\n# تحميل النموذج الأساسي\nmodel = ViltForQuestionAnswering.from_pretrained(model_name)\nmodel.config.id2label = id2answer\nmodel.config.label2id = answer2id\n# إعادة تهيئة المصنف بعدد الفئات الجديد\nclassifier_dropout = 0.1  # إضافة dropout لتقليل الـ overfitting\nhidden_size = model.config.hidden_size\nnum_labels = len(answer2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:14:02.799668Z","iopub.execute_input":"2025-02-28T04:14:02.799992Z","iopub.status.idle":"2025-02-28T04:14:10.645548Z","shell.execute_reply.started":"2025-02-28T04:14:02.799968Z","shell.execute_reply":"2025-02-28T04:14:10.644434Z"}},"outputs":[{"name":"stdout","text":"Loading dataset from JSON...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b131866cf3c24e58ad36070460a26ddd"}},"metadata":{}},{"name":"stdout","text":"Dataset loaded successfully with 20523 examples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/20523 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e3a3d3e62f74290a56c21b0b7a4811a"}},"metadata":{}},{"name":"stdout","text":"Dataset cleaned: 20484 valid examples remaining\nSplitting dataset...\nAnalyzing answer distribution...\nTop 10 answers: [('unsuitable', 21493), ('no', 4511), ('yes', 3959), ('white', 2063), ('grey', 1923), ('black', 1713), ('blue', 1562), ('red', 1011), ('pink', 721), ('brown', 703)]\nUsing 6523 answers that appear at least 3 times\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c272bc10238d413a934e9b0adc8f5843"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.1.bias', 'classifier.1.weight', 'classifier.3.bias', 'classifier.3.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# تحسين المصنف باستخدام طبقات إضافية\nclass EnhancedClassifier(nn.Module):\n    def __init__(self, hidden_size, num_labels, dropout_prob=0.1):\n        super().__init__()\n        self.dense1 = nn.Linear(hidden_size, hidden_size)\n        self.dropout1 = nn.Dropout(dropout_prob)\n        self.dense2 = nn.Linear(hidden_size, hidden_size // 2)\n        self.dropout2 = nn.Dropout(dropout_prob)\n        self.classifier = nn.Linear(hidden_size // 2, num_labels)\n        \n    def forward(self, x):\n        x = self.dropout1(F.gelu(self.dense1(x)))\n        x = self.dropout2(F.gelu(self.dense2(x)))\n        return self.classifier(x)\n\n# تطبيق المصنف المحسن\nmodel.classifier = EnhancedClassifier(hidden_size, num_labels, classifier_dropout)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:14:25.635950Z","iopub.execute_input":"2025-02-28T04:14:25.636270Z","iopub.status.idle":"2025-02-28T04:14:26.059403Z","shell.execute_reply.started":"2025-02-28T04:14:25.636244Z","shell.execute_reply":"2025-02-28T04:14:26.058597Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"ViltForQuestionAnswering(\n  (vilt): ViltModel(\n    (embeddings): ViltEmbeddings(\n      (text_embeddings): TextEmbeddings(\n        (word_embeddings): Embedding(30522, 768)\n        (position_embeddings): Embedding(40, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (patch_embeddings): ViltPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n      )\n      (token_type_embeddings): Embedding(2, 768)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViltEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViltLayer(\n          (attention): ViltAttention(\n            (attention): ViltSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViltSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViltIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViltOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViltPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (classifier): EnhancedClassifier(\n    (dense1): Linear(in_features=768, out_features=768, bias=True)\n    (dropout1): Dropout(p=0.1, inplace=False)\n    (dense2): Linear(in_features=768, out_features=384, bias=True)\n    (dropout2): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=384, out_features=6524, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# تحسين وظيفة تحميل الصور مع المعالجة المسبقة\ndef load_image(image_name):\n    image_path = os.path.join(image_folder, image_name)\n    if os.path.exists(image_path):\n        try:\n            img = Image.open(image_path).convert(\"RGB\")\n            \n            # تطبيق تقنيات معالجة الصور المتقدمة\n            img = img.resize((224, 224))\n            \n            # زيادة التباين قليلاً\n            from PIL import ImageEnhance\n            enhancer = ImageEnhance.Contrast(img)\n            img = enhancer.enhance(1.1)\n            \n            return img\n        except Exception as e:\n            print(f\"Error loading image {image_name}: {e}\")\n            return Image.new(\"RGB\", (224, 224), color=\"white\")\n    else:\n        print(f\"⚠️ Warning: Image not found - {image_name}\")\n        return Image.new(\"RGB\", (224, 224), color=\"white\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:14:37.571048Z","iopub.execute_input":"2025-02-28T04:14:37.571341Z","iopub.status.idle":"2025-02-28T04:14:37.576498Z","shell.execute_reply.started":"2025-02-28T04:14:37.571318Z","shell.execute_reply":"2025-02-28T04:14:37.575672Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# تعريف معالج خاص محسن لتجميع البيانات\n@dataclass\nclass CustomDataCollator:\n    processor: any\n    \n    def __call__(self, features):\n        if not features:\n            return {}\n            \n        batch = {}\n        \n        # معالجة الحقول\n        if \"pixel_values\" in features[0]:\n            batch[\"pixel_values\"] = torch.stack([f[\"pixel_values\"] for f in features])\n        \n        for field in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n            if field in features[0]:\n                # استخدام padding أكثر كفاءة\n                values = [f[field] for f in features]\n                max_length = max(len(v) for v in values)\n                \n                padded_values = []\n                for v in values:\n                    padding = [0] * (max_length - len(v))\n                    padded_values.append(v + padding)\n                \n                batch[field] = torch.tensor(padded_values)\n        \n        if \"labels\" in features[0]:\n            batch[\"labels\"] = torch.tensor([f[\"labels\"] for f in features])\n        \n        return batch\n\n# دالة معالجة بيانات محسنة\ndef preprocess_function(examples):\n    valid_questions = []\n    valid_images = []\n    valid_labels = []\n    \n    for i in range(len(examples.get(\"question\", []))):\n        # التحقق من وجود البيانات المطلوبة\n        if \"image\" not in examples or i >= len(examples[\"image\"]) or \"answers\" not in examples or i >= len(examples[\"answers\"]):\n            continue\n            \n        image_name = examples[\"image\"][i]\n        img = load_image(image_name)\n        \n        # معالجة الإجابات\n        has_valid_answer = False\n        if isinstance(examples[\"answers\"][i], list) and examples[\"answers\"][i]:\n            for answer in examples[\"answers\"][i]:\n                answer_text = answer.get(\"answer\", \"\") if isinstance(answer, dict) else answer\n                if answer_text and answer_text != \"unanswerable\":\n                    if answer_text in answer2id:\n                        valid_labels.append(answer2id[answer_text])\n                    else:\n                        # استخدام فئة \"أخرى\" للإجابات النادرة\n                        valid_labels.append(answer2id[\"other\"])\n                    has_valid_answer = True\n                    break\n        \n        if has_valid_answer:\n            valid_questions.append(examples[\"question\"][i])\n            valid_images.append(img)\n    \n    # التحقق من وجود بيانات صالحة\n    if not valid_questions:\n        return {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": [], \"labels\": []}\n    \n    # معالجة البيانات\n    try:\n        encoding = processor(\n            images=valid_images,\n            text=valid_questions,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # إضافة التسميات\n        encoding[\"labels\"] = valid_labels\n        \n        # تحويل التنسورات إلى قوائم\n        result = {k: v.tolist() if isinstance(v, torch.Tensor) else v for k, v in encoding.items()}\n        return result\n    except Exception as e:\n        print(f\"Error in preprocessing: {e}\")\n        return {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": [], \"labels\": []}\n\n# استخدام المزيد من البيانات للتدريب مع مراعاة حدود الذاكرة\nprint(\"Preparing datasets for training...\")\ntrain_sample_size = min(8000, len(train_data))  # زيادة حجم البيانات للتدريب\neval_sample_size = min(1200, len(val_data))\n\n# اختيار العينات بطريقة موزونة لتحسين تمثيل الفئات النادرة\nweighted_train_indices = []\nanswer_probabilities = {}\n\n# حساب احتمالية اختيار كل إجابة بشكل عكسي مع تكرارها\ntotal_answers = sum(answer_counts.values())\nfor answer, count in answer_counts.items():\n    answer_probabilities[answer] = 1.0 / (count / total_answers)\n\n# تطبيع الاحتمالات\nmax_prob = max(answer_probabilities.values())\nfor answer in answer_probabilities:\n    answer_probabilities[answer] /= max_prob\n\n# إنشاء قائمة بالمؤشرات الموزونة\nfor idx, example in enumerate(train_data):\n    if idx >= len(train_data):\n        break\n    \n    if \"answers\" in example and example[\"answers\"]:\n        for answer in example[\"answers\"]:\n            answer_text = answer.get(\"answer\", \"\") if isinstance(answer, dict) else answer\n            if answer_text in answer_probabilities:\n                # إضافة المؤشر بناءً على الاحتمالية\n                if random.random() < answer_probabilities[answer_text]:\n                    weighted_train_indices.append(idx)\n                    break\n\n# التأكد من أن لدينا ما يكفي من البيانات\nif len(weighted_train_indices) < train_sample_size:\n    # إضافة مؤشرات عشوائية إضافية إذا لزم الأمر\n    additional_indices = random.sample(\n        [i for i in range(len(train_data)) if i not in weighted_train_indices],\n        min(train_sample_size - len(weighted_train_indices), len(train_data) - len(weighted_train_indices))\n    )\n    weighted_train_indices.extend(additional_indices)\n\n# التأكد من عدم تجاوز الحد الأقصى\nweighted_train_indices = weighted_train_indices[:train_sample_size]\neval_indices = random.sample(range(len(val_data)), eval_sample_size)\n\nsmall_train_dataset = train_data.select(weighted_train_indices)\nsmall_eval_dataset = val_data.select(eval_indices)\n\nprint(f\"Selected {len(small_train_dataset)} examples for training\")\nprint(f\"Selected {len(small_eval_dataset)} examples for evaluation\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:14:58.563560Z","iopub.execute_input":"2025-02-28T04:14:58.563918Z","iopub.status.idle":"2025-02-28T04:15:01.309933Z","shell.execute_reply.started":"2025-02-28T04:14:58.563894Z","shell.execute_reply":"2025-02-28T04:15:01.309199Z"}},"outputs":[{"name":"stdout","text":"Preparing datasets for training...\nSelected 8000 examples for training\nSelected 1200 examples for evaluation\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\nprint(\"Preprocessing training data...\")\ntrain_dataset = small_train_dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=4,  # تحسين حجم الدفعة\n    remove_columns=train_data.column_names\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:15:06.163632Z","iopub.execute_input":"2025-02-28T04:15:06.163963Z","iopub.status.idle":"2025-02-28T04:31:20.028526Z","shell.execute_reply.started":"2025-02-28T04:15:06.163936Z","shell.execute_reply":"2025-02-28T04:31:20.027486Z"}},"outputs":[{"name":"stdout","text":"Preprocessing training data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2457707813934434a3703fbb6f0360a3"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"print(\"Preprocessing evaluation data...\")\neval_dataset = small_eval_dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=4,\n    remove_columns=val_data.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:34:44.484059Z","iopub.execute_input":"2025-02-28T04:34:44.484418Z","iopub.status.idle":"2025-02-28T04:37:07.550971Z","shell.execute_reply.started":"2025-02-28T04:34:44.484387Z","shell.execute_reply":"2025-02-28T04:37:07.550209Z"}},"outputs":[{"name":"stdout","text":"Preprocessing evaluation data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ee9431e590413ba6ad289760f2489a"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"\n# تنظيف البيانات\ntrain_dataset = train_dataset.filter(lambda x: len(x[\"input_ids\"]) > 0)\neval_dataset = eval_dataset.filter(lambda x: len(x[\"input_ids\"]) > 0)\n\nprint(f\"Preprocessed training examples: {len(train_dataset)}\")\nprint(f\"Preprocessed evaluation examples: {len(eval_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:38:20.952825Z","iopub.execute_input":"2025-02-28T04:38:20.953147Z","iopub.status.idle":"2025-02-28T05:21:35.595164Z","shell.execute_reply.started":"2025-02-28T04:38:20.953123Z","shell.execute_reply":"2025-02-28T05:21:35.594456Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b234ad4a15f5444e94addee7e4dd1dec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4369bfc73dca4d6eb7afd691a89ebff9"}},"metadata":{}},{"name":"stdout","text":"Preprocessed training examples: 8000\nPreprocessed evaluation examples: 1200\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install evaluate\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:23:56.930947Z","iopub.execute_input":"2025-02-28T05:23:56.931290Z","iopub.status.idle":"2025-02-28T05:24:02.309547Z","shell.execute_reply.started":"2025-02-28T05:23:56.931261Z","shell.execute_reply":"2025-02-28T05:24:02.308573Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n\n    # ✅ تحويل القيم الاحتمالية إلى التصنيفات المتوقعة\n    predicted_classes = np.argmax(logits, axis=1)\n\n    # ✅ تحويل `one-hot encoding` إلى أرقام صحيحة\n    true_classes = np.argmax(labels, axis=1)\n\n    # ✅ حساب الدقة\n    acc = accuracy.compute(predictions=predicted_classes, references=true_classes)\n\n    return {\"accuracy\": acc[\"accuracy\"]}\n\nclass CustomDataCollator:\n    def __init__(self, processor, num_classes=6524):  # حدد عدد الفئات\n        self.processor = processor\n        self.num_classes = num_classes\n\n    def __call__(self, features):\n        batch = {}\n\n        batch[\"pixel_values\"] = torch.stack(\n            [torch.tensor(f[\"pixel_values\"]) if isinstance(f[\"pixel_values\"], list) else f[\"pixel_values\"]\n             for f in features]\n        )\n\n        batch[\"input_ids\"] = torch.stack(\n            [torch.tensor(f[\"input_ids\"]) if isinstance(f[\"input_ids\"], list) else f[\"input_ids\"]\n             for f in features]\n        )\n\n        batch[\"attention_mask\"] = torch.stack(\n            [torch.tensor(f[\"attention_mask\"]) if isinstance(f[\"attention_mask\"], list) else f[\"attention_mask\"]\n             for f in features]\n        )\n\n        # ✅ تحويل `labels` إلى One-Hot\n        labels = [f[\"labels\"] for f in features]\n        labels_tensor = torch.zeros((len(labels), self.num_classes))  # إنشاء مصفوفة أصفار\n        labels_tensor.scatter_(1, torch.tensor(labels).unsqueeze(1), 1)  # تحويل إلى One-Hot\n\n        batch[\"labels\"] = labels_tensor\n\n        return batch\n\n\n\n# إنشاء معالج تجميع البيانات\ndata_collator = CustomDataCollator(processor=processor)\n\n# تعيين معلمات التدريب\ntraining_args = TrainingArguments(\n    output_dir=\"./vqa_finetuned_model\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    num_train_epochs=3,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    fp16=True,\n    gradient_accumulation_steps=4,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    report_to=\"none\",\n    remove_unused_columns=False,\n    warmup_steps=200,\n    dataloader_num_workers=2,\n    lr_scheduler_type=\"cosine\",\n)\n\nclass MixupTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.mixup_alpha = 0.2  # معامل المزج\n        \n    def training_step(self, model, inputs, num_items_in_batch):\n        \"\"\"تطبيق تقنية المزج على دفعة التدريب\"\"\"\n        if self.mixup_alpha > 0 and \"pixel_values\" in inputs and \"labels\" in inputs:\n            # تطبيق المزج بين الصور\n            batch_size = inputs[\"pixel_values\"].size(0)\n            if batch_size > 1:  # نحتاج على الأقل صورتين للمزج\n                # توليد معاملات المزج\n                lam = np.random.beta(self.mixup_alpha, self.mixup_alpha, batch_size)\n                lam = torch.from_numpy(lam).float().to(inputs[\"pixel_values\"].device)\n                lam = lam.view(-1, 1, 1, 1)\n                \n                # تشويش الفهارس\n                index = torch.randperm(batch_size).to(inputs[\"pixel_values\"].device)\n                \n                # مزج الصور\n                mixed_pixel_values = lam * inputs[\"pixel_values\"] + (1 - lam) * inputs[\"pixel_values\"][index, :]\n                inputs[\"pixel_values\"] = mixed_pixel_values\n                \n                # لا نقوم بمزج التسميات، بل نستخدم التسمية الأصلية (هذا يكفي لمهمة VQA)\n        \n        # التأكد من وجود input_ids أو inputs_embeds\n        if \"input_ids\" not in inputs and \"inputs_embeds\" not in inputs:\n            raise ValueError(\"يجب توفير input_ids أو inputs_embeds للنموذج\")\n            \n        # استدعاء دالة training_step الأصلية وتمرير جميع الوسائط المطلوبة\n        return super().training_step(model, inputs, num_items_in_batch)\n\n\n\n# إعداد المدرب\ntrainer = MixupTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\n# ✅ تجميد الطبقات السفلى لتسريع التدريب\nfor name, param in model.vilt.named_parameters():\n    if \"encoder.layer.0\" in name or \"encoder.layer.1\" in name or \"encoder.layer.2\" in name:\n        param.requires_grad = False\n\n# عرض المعلمات القابلة للتدريب\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:24:09.073058Z","iopub.execute_input":"2025-02-28T05:24:09.073398Z","iopub.status.idle":"2025-02-28T05:24:09.664741Z","shell.execute_reply.started":"2025-02-28T05:24:09.073371Z","shell.execute_reply":"2025-02-28T05:24:09.663919Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04fcffba68ac46dd9b7ce7b5903c48d7"}},"metadata":{}},{"name":"stdout","text":"Total parameters: 114,992,636\nTrainable parameters: 79,553,276 (69.18%)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(\"training ...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:24:42.044783Z","iopub.execute_input":"2025-02-28T05:24:42.045138Z","iopub.status.idle":"2025-02-28T07:07:15.466920Z","shell.execute_reply.started":"2025-02-28T05:24:42.045109Z","shell.execute_reply":"2025-02-28T07:07:15.466120Z"}},"outputs":[{"name":"stdout","text":"training ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 1:42:18, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>4348.181600</td>\n      <td>4297.930176</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>491.300700</td>\n      <td>254.207611</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>8.446400</td>\n      <td>6.272337</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>7.588500</td>\n      <td>5.873587</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>7.689600</td>\n      <td>5.678341</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>7.055200</td>\n      <td>5.717703</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>7.628700</td>\n      <td>5.682053</td>\n      <td>0.347500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=750, training_loss=953.8862706197103, metrics={'train_runtime': 6153.0314, 'train_samples_per_second': 3.901, 'train_steps_per_second': 0.122, 'total_flos': 527143380480000.0, 'train_loss': 953.8862706197103, 'epoch': 3.0})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# حفظ النموذج النهائي\nprint(\"Saving final model...\")\nmodel.save_pretrained(\"./vqa_finetuned_model_final\")\nprocessor.save_pretrained(\"./vqa_finetuned_model_final\")\nprint(\"Model saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:22:45.118140Z","iopub.execute_input":"2025-02-28T07:22:45.118541Z","iopub.status.idle":"2025-02-28T07:22:46.572053Z","shell.execute_reply.started":"2025-02-28T07:22:45.118512Z","shell.execute_reply":"2025-02-28T07:22:46.571238Z"}},"outputs":[{"name":"stdout","text":"Saving final model...\nModel saved successfully!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# كود محسن للتحقق من أداء النموذج\ndef test_model(image_name, question):\n    img = load_image(image_name)\n    inputs = processor(img, question, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # الحصول على أفضل 3 إجابات محتملة\n    logits = outputs.logits\n    probs = F.softmax(logits, dim=-1)[0]\n    top_probs, top_indices = probs.topk(3)\n    \n    results = []\n    for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):\n        answer = model.config.id2label.get(idx, \"غير معروف\")\n        results.append((answer, prob * 100))\n    \n    print(f\"سؤال: {question}\")\n    print(f\"الإجابة الأكثر احتمالا: {results[0][0]} (الثقة: {results[0][1]:.2f}%)\")\n    print(f\"إجابات أخرى محتملة: {results[1][0]} ({results[1][1]:.2f}%), {results[2][0]} ({results[2][1]:.2f}%)\")\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:24:18.597055Z","iopub.execute_input":"2025-02-28T07:24:18.597404Z","iopub.status.idle":"2025-02-28T07:24:18.604214Z","shell.execute_reply.started":"2025-02-28T07:24:18.597379Z","shell.execute_reply":"2025-02-28T07:24:18.603303Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n# دالة للتحقق من دقة النموذج على مجموعة البيانات\ndef evaluate_model_accuracy(dataset, num_samples=100):\n    if len(dataset) == 0:\n        print(\"لا توجد بيانات للتقييم\")\n        return 0\n    \n    # اختيار عينة عشوائية للتقييم\n    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n    correct = 0\n    \n    for idx in indices:\n        example = dataset[idx]\n        if \"image\" not in example or \"question\" not in example or \"answers\" not in example:\n            continue\n            \n        img = load_image(example[\"image\"])\n        question = example[\"question\"]\n        \n        # الحصول على الإجابة الصحيحة\n        correct_answer = None\n        if isinstance(example[\"answers\"], list) and example[\"answers\"]:\n            answer = example[\"answers\"][0]\n            correct_answer = answer.get(\"answer\", \"\") if isinstance(answer, dict) else answer\n        \n        if not correct_answer or correct_answer == \"unanswerable\":\n            continue\n            \n        # التنبؤ بالإجابة\n        inputs = processor(img, question, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=40)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n        \n        # الحصول على الإجابة المتوقعة\n        logits = outputs.logits\n        predicted_answer_id = logits.argmax(-1).item()\n        predicted_answer = model.config.id2label.get(predicted_answer_id, \"غير معروف\")\n        \n        # مقارنة الإجابة المتوقعة بالصحيحة\n        if predicted_answer.lower() == correct_answer.lower():\n            correct += 1\n    \n    accuracy = correct / len(indices) * 100\n    print(f\"دقة النموذج على {len(indices)} عينة: {accuracy:.2f}%\")\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:29:24.304407Z","iopub.execute_input":"2025-02-28T07:29:24.304772Z","iopub.status.idle":"2025-02-28T07:29:24.312365Z","shell.execute_reply.started":"2025-02-28T07:29:24.304745Z","shell.execute_reply":"2025-02-28T07:29:24.311425Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# تقييم النموذج بعد التدريب\nprint(\"Evaluating final model...\")\nfinal_accuracy = evaluate_model_accuracy(val_data, num_samples=200)\nprint(f\"Final model accuracy: {final_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:29:27.563149Z","iopub.execute_input":"2025-02-28T07:29:27.563470Z","iopub.status.idle":"2025-02-28T07:29:40.297744Z","shell.execute_reply.started":"2025-02-28T07:29:27.563446Z","shell.execute_reply":"2025-02-28T07:29:40.296747Z"}},"outputs":[{"name":"stdout","text":"Evaluating final model...\nدقة النموذج على 200 عينة: 4.00%\nFinal model accuracy: 4.00%\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"for i in range(5):\n    sample = val_data[i]\n    print(f\"🔹 السؤال: {sample['question']}\")\n    print(f\"✅ الإجابة الصحيحة: {sample['answers']}\")\n    print(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:31:50.358539Z","iopub.execute_input":"2025-02-28T07:31:50.358937Z","iopub.status.idle":"2025-02-28T07:31:50.369110Z","shell.execute_reply.started":"2025-02-28T07:31:50.358906Z","shell.execute_reply":"2025-02-28T07:31:50.368237Z"}},"outputs":[{"name":"stdout","text":"🔹 السؤال: what number is the needle pointing to?\n✅ الإجابة الصحيحة: [{'answer': 'illegible', 'answer_confidence': 'no'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'blurry', 'answer_confidence': 'no'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'no'}]\n--------------------------------------------------\n🔹 السؤال: What color is the keyboard?\n✅ الإجابة الصحيحة: [{'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'keyboard', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}, {'answer': 'black', 'answer_confidence': 'yes'}]\n--------------------------------------------------\n🔹 السؤال: What is in this bottle? What is in this bottle?\n✅ الإجابة الصحيحة: [{'answer': 'ball park mustard', 'answer_confidence': 'yes'}, {'answer': 'efrwqerf', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'yes'}, {'answer': 'ball park mustard', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'yes'}, {'answer': 'bertman original ball park mustard', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'yes'}, {'answer': 'mustard', 'answer_confidence': 'maybe'}]\n--------------------------------------------------\n🔹 السؤال: What is it say on this tag?\n✅ الإجابة الصحيحة: [{'answer': 'blank', 'answer_confidence': 'yes'}, {'answer': 'nothing', 'answer_confidence': 'maybe'}, {'answer': 'nothing', 'answer_confidence': 'yes'}, {'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'nothing', 'answer_confidence': 'yes'}, {'answer': 'nothing', 'answer_confidence': 'yes'}, {'answer': 'nothing to say', 'answer_confidence': 'yes'}, {'answer': 'nothing', 'answer_confidence': 'maybe'}, {'answer': 'nothing', 'answer_confidence': 'maybe'}, {'answer': 'nothing', 'answer_confidence': 'maybe'}]\n--------------------------------------------------\n🔹 السؤال: Yes, I just need a description of the label on this bottle. Please be as specific as possible. Thank you.\n✅ الإجابة الصحيحة: [{'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'blurry', 'answer_confidence': 'maybe'}, {'answer': 'lots information', 'answer_confidence': 'maybe'}, {'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'unsuitable', 'answer_confidence': 'no'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}, {'answer': 'too blurry to read but label light blue white has many words on', 'answer_confidence': 'maybe'}, {'answer': 'unsuitable', 'answer_confidence': 'yes'}]\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!zip -r vqa_finetuned_model_final.zip ./vqa_finetuned_model_final\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'vqa_finetuned_model_final.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
